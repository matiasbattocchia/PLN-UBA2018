{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Práctico 1](#Práctico-1)\n",
    "* [Práctico 2](#Práctico-2)\n",
    "* [Práctico 3](#Práctico-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctico 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elegir corpus de texto en lenguaje natural de más de 5Mb de tamaño.\n",
    "\n",
    "Se elegió el conjunto de reportes de experiencias [Erowid](http://erowid.org), una colección de más de 20.000 reportes personales que describen experiencias con plantas y drogas psicoactivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS_DIR = '../corpus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos archivos no están en UTF-8 válido así que los corregimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import os, io\n",
    "\n",
    "#CORPUS_SOURCE = '../icowid-generator/corpi/text'\n",
    "\n",
    "#for filename in os.listdir(CORPUS_SOURCE):\n",
    "    \n",
    "#    with io.open(CORPUS_SOURCE + '/' + filename, 'r', encoding='utf8', errors='ignore') as i:\n",
    "#        with io.open(CORPUS_DIR + '/' + 'utf8_' + filename, 'w', encoding='utf8') as o:\n",
    "#            o.write(i.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar el corpus usando un “corpus reader” de NLTK (e.g. PlaintextCorpusReader) o definiendo uno propio.\n",
    "\n",
    "El “corpus reader” debe proveer un método sents que permita iterar sobre las oraciones tokenizadas del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "#nltk.download('punkt')\n",
    "\n",
    "corpus = PlaintextCorpusReader(CORPUS_DIR, '.*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisar a ojo la tokenización y segmentado en oraciones. Si es muy mala, probar otras formas de tokenización/segmentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['At', 'first', 'it', 'was', 'just', 'another', 'rave', 'with', 'my', 'crew', '.'], ['Drop', 'a', 'little', 'E', ',', 'have', 'a', 'good', 'night', '.'], ['What', 'I', 'didn', \"'\", 't', 'know', 'was', 'this', 'was', 'going', 'to', 'be', 'one', 'rave', 'I', 'probably', 'won', \"'\", 't', 'ever', 'forget', ',', 'what', 'I', 'can', 'remember', 'that', 'is', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(list(corpus.sents(fileids=['utf8_erowid_2.txt']))[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenización parece funcionar bastante bien. Sin embargo se observan algunos defectos producto de utilizar el tokenizador por defecto basado en `nltk.tokenize.regexp.WordPunctTokenize`, que corresponde a la expresión regular `\\w+|\\[^\\w\\s]+`.\n",
    "\n",
    "* Abreviaturas: `['a', '.', 'm', '.']`\n",
    "* La hora: `['6', ':', '30']`\n",
    "\n",
    "Más algunas separaciones que podrían repararse o no dependiendo del criterio elegido. Por ahora las dejamos como están.\n",
    "\n",
    "* Palabras compuestas: `['elastic', '-', 'like']`\n",
    "* Omisiones, uso del apóstrofe en general: `['couldn', \"'\", 't']`, `['bustin', \"'\"]`\n",
    "\n",
    "Escribimos una expresión regular con más reglas.\n",
    "\n",
    "**Importante**\n",
    "\n",
    "* Para que funcione las reglas tienen que ir de más específicas a más generales.\n",
    "* Los grupos tienen que ser no capturantes `(?: ...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Son', 'las', '6:30', 'a.m.', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'''(?x)    # verbose regexps\n",
    "      \\d+[.:]\\d+      # horas y números con decimales\n",
    "    | \\w+\\.(?:\\w+\\.)+ # abreviaturas\n",
    "    | \\w+             # palabras alfanuméricas\n",
    "    | [^\\w\\s]+        # signos de puntuación\n",
    "'''\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "tokenizer.tokenize('Son las 6:30 a.m..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deja mucho que desear el filtro de archivos (argumento `fileids`) de `PlaintextCorpusReader` así que se los especificamos a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "archivos = [os.path.basename(file) for file in glob.glob(CORPUS_DIR + '/utf8_*.txt')]\n",
    "\n",
    "corpus = PlaintextCorpusReader(CORPUS_DIR, archivos, word_tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modificar el script train.py para utilizar el nuevo corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python train.py -n 1 -o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Modelo de n-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notas\n",
    "\n",
    "* `<s>` no se cuenta como unigrama.\n",
    "* La probabilidad condicional cuando hay división por cero se corrige a cero.\n",
    "* Los primeros tokens de una oración son `<s> * (n-1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "import ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp.reload(ngram)\n",
    "ng = ngram.NGram(4, corpus.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ngram_generator\n",
    "imp.reload(ngram_generator)\n",
    "\n",
    "ngg = ngram_generator.NGramGenerator(ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "\n",
    "call('rm oraciones.txt', shell=True)\n",
    "\n",
    "for n in [1,2,3,4]:\n",
    "    call('python train.py -n {n} -o modelo_n{n}.pkl'.format(n=n), shell=True)\n",
    "    call('echo \"#### Modelo n={n} ####\" >> oraciones.txt'.format(n=n), shell=True)\n",
    "    call('python generate.py -i modelo_n{n}.pkl -n 4 >> oraciones.txt'.format(n=n), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo n=1 ####\n",
    "\n",
    "* I\n",
    "* The ) I truly she the one process. Set time told just it. Of all rolled in a as\n",
    "* About anywhere cloudy mention we worse having\n",
    "* I minutes the experiences that least to I it the was this. Dance. Effects verbatim in i wasn days at could is t could\n",
    "\n",
    "#### Modelo n=2 ####\n",
    "\n",
    "* I felt that a body, j's.\n",
    "* Might be okay so much acid hell, and make out to the dealer again.\n",
    "* A druggy circle', it, and pretty much as I felt an hour or 2 - thc is the time and waited a certain doses of previous night in love for a punishment for granted and I'burned out for so she can't hungry.\n",
    "* After the right, hoping and 100mg himself.\n",
    "\n",
    "#### Modelo n=3 ####\n",
    "\n",
    "* He pointed, and we resolved to keep'.\n",
    "* Note that this was the worst part of my mates for the past, present and that the feeling of relaxation.\n",
    "* I have tried and many experiments with datura, aka jimsonweed, belladonna, etc.\n",
    "* The solvents used in that single experience, which had a deep dreamless sleep.\n",
    "\n",
    "#### Modelo n=4 ####\n",
    "\n",
    "* I was having trouble moving around at any kind of trouble or bad experiences I have had with just taking normal Adderall.\n",
    "* I started writhing on my bed.\n",
    "* I remember letting go the cloud of smoke.\n",
    "* I recognized my room, a bed room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Suavizado *add-one*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ngram\n",
    "imp.reload(ngram)\n",
    "aong = ngram.AddOneNGram(2, corpus.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Evaluación de modelos de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 6: Suavizado por interpolación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_sents = [[word.lower() for word in sent] for sent in corpus.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ngram\n",
    "imp.reload(ngram)\n",
    "ing = ngram.InterpolatedNGram(3, corpus_sents, gamma=1, addone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in [1,2,3,4]:\n",
    "    ngram.InterpolatedNGram(n, corpus_sents, addone=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "n | gamma | perplejidad\n",
    "-----------------------\n",
    "1 | 0.01  | 542\n",
    "2 |  100  | 172\n",
    "3 |  100  | 161\n",
    "4 | 1000  | 163\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
